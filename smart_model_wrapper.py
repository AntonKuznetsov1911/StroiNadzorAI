"""
Wrapper –¥–ª—è —É–º–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ handle_text –∏ handle_photo
–ï—Å–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–Ω, –∏–Ω–∞—á–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –æ–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Grok
"""

import logging
from typing import Optional, Dict
import asyncio

logger = logging.getLogger(__name__)


async def smart_model_selection_text(
    question: str,
    user_id: int,
    thinking_message,
    update,
    context
) -> Optional[Dict]:
    """
    –£–º–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∏–ª–∏ None (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Grok)
    """
    try:
        from model_selector import ModelSelector
        from optimized_handlers import handle_with_claude_technical, handle_with_gemini_image, handle_with_grok
        from optimized_prompts import CLAUDE_SYSTEM_PROMPT_TECHNICAL, GEMINI_IMAGE_PROMPT_SYSTEM, GROK_SYSTEM_PROMPT_GENERAL
        from history_manager import get_user_history, add_message_to_history_async

        selector = ModelSelector()
        decision = selector.classify_request(question, has_photo=False)

        logger.info(f"ü§ñ –£–º–Ω—ã–π –≤—ã–±–æ—Ä: {decision['model']}")
        logger.info(f"üí° {decision['reason']}")
        logger.info(f"üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å: ${decision['estimated_cost']:.3f}")

        # CLAUDE - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        if decision["model"] == "claude_technical":
            try:
                conversation_history = await get_user_history(user_id, limit=10)

                answer = await handle_with_claude_technical(
                    question=question,
                    user_id=user_id,
                    conversation_history=conversation_history,
                    system_prompt=CLAUDE_SYSTEM_PROMPT_TECHNICAL
                )

                # –£–¥–∞–ª—è–µ–º thinking message
                try:
                    await thinking_message.delete()
                except:
                    pass

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                await update.message.reply_text(
                    f"{answer}\n\n_‚ú® Claude Sonnet 4.5_",
                    parse_mode="Markdown"
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                await add_message_to_history_async(user_id, 'assistant', answer)

                logger.info("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω (Claude)")
                return {"success": True, "model": "claude"}

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Claude: {e}")
                return None  # Fallback –Ω–∞ Grok

        # GEMINI - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä—Ç–µ–∂–µ–π
        elif decision["model"] == "gemini_image":
            try:
                await thinking_message.edit_text(
                    "üé® –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —á–µ—Ä—Ç—ë–∂...\n\n"
                    "üü£ Gemini 2.5 Flash Image —Å–æ–∑–¥–∞—ë—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
                )

                result = await handle_with_gemini_image(
                    question=question,
                    image_prompt_system=GEMINI_IMAGE_PROMPT_SYSTEM
                )

                try:
                    await thinking_message.delete()
                except:
                    pass

                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ
                if result.get('image_data'):
                    result['image_data'].seek(0)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ

                    caption = f"üé® **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —á–µ—Ä—Ç—ë–∂**\n\n"
                    if result.get('description'):
                        caption += f"{result['description']}\n\n"
                    caption += "_‚ú® Gemini 2.5 Flash Image_"

                    await update.message.reply_photo(
                        photo=result['image_data'],
                        caption=caption,
                        parse_mode="Markdown"
                    )

                    await add_message_to_history_async(user_id, 'assistant', f"[–ß–µ—Ä—Ç—ë–∂ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {question}]")
                    logger.info("‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ (Gemini)")
                else:
                    # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                    await update.message.reply_text(
                        f"üìê **–¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –û–ü–ò–°–ê–ù–ò–ï**\n\n{result.get('description', '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ')}\n\n_‚ú® Gemini 2.5 Flash_",
                        parse_mode="Markdown"
                    )

                    await add_message_to_history_async(user_id, 'assistant', f"[–û–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä—Ç–µ–∂–∞: {question}]")
                    logger.info("‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ (Gemini)")

                return {"success": True, "model": "gemini_image"}

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä—Ç–µ–∂–∞: {e}")
                return None

        # GROK - –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ web search
        elif decision["model"] == "grok_general":
            try:
                conversation_history = await get_user_history(user_id, limit=10)

                answer = await handle_with_grok(
                    question=question,
                    user_id=user_id,
                    conversation_history=conversation_history,
                    system_prompt=GROK_SYSTEM_PROMPT_GENERAL,
                    needs_web_search=decision.get("needs_web_search", False)
                )

                # –£–¥–∞–ª—è–µ–º thinking message
                try:
                    await thinking_message.delete()
                except:
                    pass

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                await update.message.reply_text(
                    f"{answer}\n\n_‚ú® Grok {'(Web Search)' if decision.get('needs_web_search') else ''}_",
                    parse_mode="Markdown"
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                await add_message_to_history_async(user_id, 'assistant', answer)

                logger.info("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω (Grok)")
                return {"success": True, "model": "grok"}

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Grok: {e}")
                return None  # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫

        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        return None  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–º

    except ImportError:
        logger.warning("‚ö†Ô∏è –ú–æ–¥—É–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Grok")
        return None
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–º–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞: {e}")
        return None


async def smart_model_selection_photo(
    question: str,
    photo_file_id: str,
    update,
    context
) -> Optional[Dict]:
    """
    –£–º–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–æ—Ç–æ

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∏–ª–∏ None (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Grok)
    """
    try:
        from model_selector import ModelSelector
        from optimized_handlers import handle_with_gemini_vision
        from optimized_prompts import GEMINI_VISION_PROMPT_DEFECTS

        selector = ModelSelector()
        decision = selector.classify_request(question, has_photo=True)

        logger.info(f"üì∏ –£–º–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è —Ñ–æ—Ç–æ: {decision['model']}")

        # GEMINI - –∞–Ω–∞–ª–∏–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤
        if decision["model"] == "gemini_vision":
            try:
                analysis = await handle_with_gemini_vision(
                    question=question,
                    photo_file_id=photo_file_id,
                    bot=context.bot,
                    system_prompt=GEMINI_VISION_PROMPT_DEFECTS
                )

                await update.message.reply_text(
                    f"{analysis}\n\n_‚ú® Gemini Vision_",
                    parse_mode="Markdown"
                )

                logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω (Gemini)")
                return {"success": True, "model": "gemini"}

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}")
                return None

        return None  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å Grok

    except ImportError:
        return None
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–æ—Ç–æ: {e}")
        return None
