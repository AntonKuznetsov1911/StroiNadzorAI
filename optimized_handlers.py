"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π
–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ bot.py –¥–ª—è —É–º–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–¥–∞—á
"""

import logging
import asyncio
from typing import Dict, Optional
import os

logger = logging.getLogger(__name__)


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö GROK (–ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã, web search)
# ============================================================================

async def handle_with_grok(
    question: str,
    user_id: int,
    conversation_history: list,
    system_prompt: str,
    needs_web_search: bool = False
) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ xAI Grok

    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        needs_web_search: –ù—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç Grok
    """
    logger.info(f"üü¢ –ò—Å–ø–æ–ª—å–∑—É–µ–º Grok {'—Å web search' if needs_web_search else '–±–µ–∑ web search'}")

    try:
        from xai_client import XAIClient

        xai_client = XAIClient(api_key=os.getenv("XAI_API_KEY"))

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = [{"role": "system", "content": system_prompt}]

        for msg in conversation_history:
            if msg.get("role") != "system":
                messages.append(msg)

        messages.append({"role": "user", "content": question})

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
        search_parameters = None
        if needs_web_search:
            search_parameters = [{"type": "web_search"}]

        # –í—ã–∑—ã–≤–∞–µ–º Grok
        response = await xai_client.chat_completions_create_async(
            model="grok-2-latest",
            messages=messages,
            max_tokens=2000,
            temperature=0.7,
            search_parameters=search_parameters
        )

        answer = response["choices"][0]["message"]["content"]

        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç Grok ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)")

        return answer

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Grok: {e}")
        raise


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö CLAUDE (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã)
# ============================================================================

async def handle_with_claude_technical(
    question: str,
    user_id: int,
    conversation_history: list,
    system_prompt: str
) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ Claude Sonnet 4.5

    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç Claude
    """
    logger.info("üîµ –ò—Å–ø–æ–ª—å–∑—É–µ–º Claude Sonnet 4.5 –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞")

    try:
        from anthropic import Anthropic

        claude_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = []
        for msg in conversation_history:
            if msg.get("role") != "system":
                messages.append(msg)

        messages.append({"role": "user", "content": question})

        # –í—ã–∑—ã–≤–∞–µ–º Claude –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()

        def _call_claude():
            response = claude_client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=2500,
                temperature=0.7,
                system=system_prompt,
                messages=messages
            )
            return response.content[0].text

        answer = await loop.run_in_executor(None, _call_claude)

        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç Claude ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)")

        return answer

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Claude: {e}")
        raise


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö GEMINI VISION (–∞–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ –¥–µ—Ñ–µ–∫—Ç–æ–≤)
# ============================================================================

async def handle_with_gemini_vision(
    question: str,
    photo_file_id: str,
    bot,
    system_prompt: str
) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ Gemini Vision

    Args:
        question: –í–æ–ø—Ä–æ—Å/–æ–ø–∏—Å–∞–Ω–∏–µ –∫ —Ñ–æ—Ç–æ
        photo_file_id: ID —Ñ–æ—Ç–æ –≤ Telegram
        bot: Telegram bot instance
        system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

    Returns:
        –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –¥–µ—Ñ–µ–∫—Ç–∞–º
    """
    logger.info("üü£ –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini Vision –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–∞ –Ω–∞ —Ñ–æ—Ç–æ")

    try:
        import google.generativeai as genai
        import base64
        from PIL import Image
        from io import BytesIO

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º Gemini
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–æ—Ç–æ
        file = await bot.get_file(photo_file_id)
        photo_bytes = await file.download_as_bytearray()

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        image = Image.open(BytesIO(photo_bytes))
        buffered = BytesIO()
        image.save(buffered, format="JPEG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        full_prompt = f"{system_prompt}\n\n–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {question}"

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()

        def _call_gemini():
            response = model.generate_content([
                full_prompt,
                {"mime_type": "image/jpeg", "data": img_base64}
            ])
            return response.text

        analysis = await loop.run_in_executor(None, _call_gemini)

        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤ –æ—Ç Gemini ({len(analysis)} —Å–∏–º–≤–æ–ª–æ–≤)")

        return analysis

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}")
        raise


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö GEMINI IMAGE (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä—Ç–µ–∂–µ–π)
# ============================================================================

async def handle_with_gemini_image(
    question: str,
    image_prompt_system: str
) -> Dict:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —á–µ—Ä—Ç–µ–∂–∞ —á–µ—Ä–µ–∑ Gemini 2.5 Flash

    Args:
        question: –û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –Ω—É–∂–Ω–æ –Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å
        image_prompt_system: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    Returns:
        Dict —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ –æ–ø–∏—Å–∞–Ω–∏–µ–º
    """
    logger.info("üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä—Ç–µ–∂–∞ —á–µ—Ä–µ–∑ Gemini 2.5 Flash")

    try:
        import google.generativeai as genai
        import base64
        from io import BytesIO
        from PIL import Image as PILImage

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º Gemini
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        full_prompt = f"""{image_prompt_system}

–ó–ê–î–ê–ß–ê: {question}

–°–æ–∑–¥–∞–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —á–µ—Ä—Ç—ë–∂ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –ì–û–°–¢ –† 2.109-2023."""

        logger.info("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä—Ç—ë–∂ —á–µ—Ä–µ–∑ Gemini...")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        loop = asyncio.get_event_loop()

        def _call_gemini():
            response = model.generate_content(
                full_prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.4,
                    top_p=0.8,
                    top_k=40,
                    max_output_tokens=2048,
                )
            )
            return response.text

        result_text = await loop.run_in_executor(None, _call_gemini)

        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –æ—Ç Gemini –ø–æ–ª—É—á–µ–Ω")

        # –¢–∞–∫ –∫–∞–∫ Gemini 2.0 Flash –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é,
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è, –∞ –∑–∞—Ç–µ–º
        # –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏–ª–∏ –≤–µ—Ä–Ω—É—Ç—å —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ

        return {
            "description": result_text,
            "image_url": None,  # Gemini 2.0 Flash –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            "prompt_used": full_prompt,
            "note": "Gemini 2.0 Flash —Å–æ–∑–¥–∞–ª –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä—Ç–µ–∂–∞"
        }

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Gemini: {e}")
        raise


# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def format_claude_response_for_telegram(text: str) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ Claude –¥–ª—è Telegram
    """
    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    return text


def format_gemini_response_for_telegram(text: str) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ Gemini –¥–ª—è Telegram
    """
    return text
