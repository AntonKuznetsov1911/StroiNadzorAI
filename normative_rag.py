"""
–°—Ç—Ä–æ–π–ù–∞–¥–∑–æ—ÄAI - Normative RAG System
=====================================
Retrieval-Augmented Generation –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –†–§

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- Vector DB (ChromaDB) –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- PDF Parser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
- Chunker –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –ø—É–Ω–∫—Ç—ã
- Semantic Search –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ—Ä–º
- Integration API –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –±–æ—Ç–µ
"""

import os
import json
import hashlib
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

RAG_CONFIG = {
    "chunk_size": 500,           # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    "chunk_overlap": 50,         # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤
    "top_k": 5,                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    "min_relevance_score": 0.7,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    "embedding_model": "text-embedding-3-small",  # OpenAI –º–æ–¥–µ–ª—å
    "collection_name": "stroinadzor_norms",
}

# –ü—É—Ç—å –∫ –±–∞–∑–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
NORMS_DIR = Path(__file__).parent / "norms_database"
CHROMA_DIR = Path(__file__).parent / "chroma_db"


# ============================================================================
# –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–•
# ============================================================================

@dataclass
class NormativeDocument:
    """–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
    id: str                      # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID (hash)
    code: str                    # –ö–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–°–ü 63.13330.2018)
    title: str                   # –ù–∞–∑–≤–∞–Ω–∏–µ
    category: str                # –ö–∞—Ç–µ–≥–æ—Ä–∏—è (–°–ü, –ì–û–°–¢, –°–ù–∏–ü, –§–ó)
    status: str                  # –°—Ç–∞—Ç—É—Å (active, superseded, cancelled)
    effective_date: Optional[str]  # –î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è
    source_file: Optional[str]   # –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª


@dataclass
class NormativeChunk:
    """–§—Ä–∞–≥–º–µ–Ω—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    id: str                      # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —á–∞–Ω–∫–∞
    document_id: str             # ID —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    document_code: str           # –ö–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    section: str                 # –†–∞–∑–¥–µ–ª/–ø—É–Ω–∫—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä "5.2.1")
    content: str                 # –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞
    page: Optional[int]          # –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    metadata: Dict               # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    chunk: NormativeChunk
    relevance_score: float
    document: NormativeDocument


# ============================================================================
# VECTOR DATABASE (ChromaDB)
# ============================================================================

class NormativeVectorDB:
    """–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤"""

    def __init__(self):
        self.client = None
        self.collection = None
        self.openai_client = None
        self._initialized = False

    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            import chromadb
            from chromadb.config import Settings

            # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            CHROMA_DIR.mkdir(parents=True, exist_ok=True)

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB —Å –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º
            self.client = chromadb.PersistentClient(
                path=str(CHROMA_DIR),
                settings=Settings(anonymized_telemetry=False)
            )

            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.collection = self.client.get_or_create_collection(
                name=RAG_CONFIG["collection_name"],
                metadata={"description": "–°—Ç—Ä–æ–π–ù–∞–¥–∑–æ—ÄAI –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –†–§"}
            )

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            from openai import OpenAI
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            self._initialized = True
            logger.info(f"‚úÖ NormativeVectorDB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.collection.count()}")
            return True

        except ImportError as e:
            logger.error(f"‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install chromadb openai")
            return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ VectorDB: {e}")
            return False

    def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI"""
        if not self.openai_client:
            raise RuntimeError("OpenAI client –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        response = self.openai_client.embeddings.create(
            model=RAG_CONFIG["embedding_model"],
            input=text
        )
        return response.data[0].embedding

    def _get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        if not self.openai_client:
            raise RuntimeError("OpenAI client –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        response = self.openai_client.embeddings.create(
            model=RAG_CONFIG["embedding_model"],
            input=texts
        )
        return [item.embedding for item in response.data]

    def add_chunks(self, chunks: List[NormativeChunk]) -> int:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É"""
        if not self._initialized:
            raise RuntimeError("VectorDB –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

        if not chunks:
            return 0

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        ids = [chunk.id for chunk in chunks]
        documents = [chunk.content for chunk in chunks]
        metadatas = [
            {
                "document_id": chunk.document_id,
                "document_code": chunk.document_code,
                "section": chunk.section,
                "page": chunk.page or 0,
                **chunk.metadata
            }
            for chunk in chunks
        ]

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        batch_size = 100
        embeddings = []
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_embeddings = self._get_embeddings_batch(batch)
            embeddings.extend(batch_embeddings)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )

        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ VectorDB")
        return len(chunks)

    def search(self, query: str, top_k: int = None, filter_dict: Dict = None) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ"""
        if not self._initialized:
            raise RuntimeError("VectorDB –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

        top_k = top_k or RAG_CONFIG["top_k"]

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self._get_embedding(query)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=filter_dict,
            include=["documents", "metadatas", "distances"]
        )

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        search_results = []
        if results and results["ids"] and results["ids"][0]:
            for i, doc_id in enumerate(results["ids"][0]):
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º distance –≤ relevance score (ChromaDB –∏—Å–ø–æ–ª—å–∑—É–µ—Ç L2)
                distance = results["distances"][0][i] if results["distances"] else 0
                relevance = 1 / (1 + distance)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ score 0-1

                if relevance >= RAG_CONFIG["min_relevance_score"]:
                    search_results.append({
                        "id": doc_id,
                        "content": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i],
                        "relevance_score": round(relevance, 3)
                    })

        return search_results

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        if not self._initialized:
            return {"status": "not_initialized"}

        return {
            "status": "active",
            "total_chunks": self.collection.count(),
            "collection_name": RAG_CONFIG["collection_name"]
        }


# ============================================================================
# PDF PARSER
# ============================================================================

class NormativePDFParser:
    """–ü–∞—Ä—Å–µ—Ä PDF –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    @staticmethod
    def parse_pdf(file_path: str) -> Tuple[str, List[Dict]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
        Returns: (full_text, pages_list)
        """
        try:
            import fitz  # PyMuPDF

            doc = fitz.open(file_path)
            pages = []
            full_text = ""

            for page_num, page in enumerate(doc, 1):
                text = page.get_text()
                pages.append({
                    "page": page_num,
                    "text": text
                })
                full_text += f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ---\n{text}"

            doc.close()
            return full_text, pages

        except ImportError:
            logger.error("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è: pip install pymupdf")
            return "", []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF {file_path}: {e}")
            return "", []

    @staticmethod
    def extract_document_info(text: str, filename: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        import re

        info = {
            "code": "",
            "title": "",
            "category": "unknown",
            "effective_date": None
        }

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç—É
        filename_upper = filename.upper()
        if "–°–ü " in filename_upper or "SP " in filename_upper:
            info["category"] = "–°–ü"
        elif "–ì–û–°–¢" in filename_upper or "GOST" in filename_upper:
            info["category"] = "–ì–û–°–¢"
        elif "–°–ù–ò–ü" in filename_upper or "SNIP" in filename_upper:
            info["category"] = "–°–ù–∏–ü"
        elif "–§–ó" in filename_upper or "FZ" in filename_upper:
            info["category"] = "–§–ó"
        elif "–ü–£–≠" in filename_upper:
            info["category"] = "–ü–£–≠"

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        patterns = [
            r"(–°–ü\s*\d+\.\d+\.\d+)",
            r"(–ì–û–°–¢\s*[\d\.\-]+)",
            r"(–°–ù–∏–ü\s*[\d\.\-]+)",
            r"(–§–ó[\-\s]*\d+)",
        ]

        for pattern in patterns:
            match = re.search(pattern, text[:2000], re.IGNORECASE)
            if match:
                info["code"] = match.group(1).strip()
                break

        if not info["code"]:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            info["code"] = Path(filename).stem

        return info


# ============================================================================
# CHUNKER
# ============================================================================

class NormativeChunker:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏"""

    @staticmethod
    def chunk_by_sections(text: str, document_id: str, document_code: str) -> List[NormativeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º/–ø—É–Ω–∫—Ç–∞–º"""
        import re

        chunks = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—É–Ω–∫—Ç–æ–≤
        section_pattern = r"(\d+\.\d+(?:\.\d+)?)\s+"

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—É–Ω–∫—Ç–∞–º
        parts = re.split(section_pattern, text)

        current_section = "0"
        current_text = ""

        for i, part in enumerate(parts):
            if re.match(r"^\d+\.\d+", part):
                # –≠—Ç–æ –Ω–æ–º–µ—Ä –ø—É–Ω–∫—Ç–∞
                if current_text.strip():
                    chunk_id = hashlib.md5(
                        f"{document_id}_{current_section}_{current_text[:50]}".encode()
                    ).hexdigest()[:16]

                    chunks.append(NormativeChunk(
                        id=chunk_id,
                        document_id=document_id,
                        document_code=document_code,
                        section=current_section,
                        content=current_text.strip(),
                        page=None,
                        metadata={"type": "section"}
                    ))

                current_section = part
                current_text = ""
            else:
                current_text += part

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_text.strip():
            chunk_id = hashlib.md5(
                f"{document_id}_{current_section}_{current_text[:50]}".encode()
            ).hexdigest()[:16]

            chunks.append(NormativeChunk(
                id=chunk_id,
                document_id=document_id,
                document_code=document_code,
                section=current_section,
                content=current_text.strip(),
                page=None,
                metadata={"type": "section"}
            ))

        # –ï—Å–ª–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—É–Ω–∫—Ç–∞–º –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –¥–µ–ª–∞–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
        if len(chunks) < 3:
            chunks = NormativeChunker.chunk_by_size(
                text, document_id, document_code,
                chunk_size=RAG_CONFIG["chunk_size"],
                overlap=RAG_CONFIG["chunk_overlap"]
            )

        return chunks

    @staticmethod
    def chunk_by_size(
        text: str,
        document_id: str,
        document_code: str,
        chunk_size: int = 500,
        overlap: int = 50
    ) -> List[NormativeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
        chunks = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        sentences = text.replace('\n', ' ').split('. ')

        current_chunk = ""
        chunk_num = 0

        for sentence in sentences:
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk.strip():
                    chunk_id = hashlib.md5(
                        f"{document_id}_chunk_{chunk_num}".encode()
                    ).hexdigest()[:16]

                    chunks.append(NormativeChunk(
                        id=chunk_id,
                        document_id=document_id,
                        document_code=document_code,
                        section=f"chunk_{chunk_num}",
                        content=current_chunk.strip(),
                        page=None,
                        metadata={"type": "size_chunk", "chunk_num": chunk_num}
                    ))
                    chunk_num += 1

                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                words = current_chunk.split()
                overlap_text = " ".join(words[-overlap:]) if len(words) > overlap else ""
                current_chunk = overlap_text + " " + sentence + ". "

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunk_id = hashlib.md5(
                f"{document_id}_chunk_{chunk_num}".encode()
            ).hexdigest()[:16]

            chunks.append(NormativeChunk(
                id=chunk_id,
                document_id=document_id,
                document_code=document_code,
                section=f"chunk_{chunk_num}",
                content=current_chunk.strip(),
                page=None,
                metadata={"type": "size_chunk", "chunk_num": chunk_num}
            ))

        return chunks


# ============================================================================
# RAG SEARCH API
# ============================================================================

class NormativeRAG:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤"""

    def __init__(self):
        self.vector_db = NormativeVectorDB()
        self.documents: Dict[str, NormativeDocument] = {}
        self._initialized = False

    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        if self.vector_db.initialize():
            self._initialized = True
            self._load_documents_index()
            logger.info("‚úÖ NormativeRAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return True
        return False

    def _load_documents_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        index_file = NORMS_DIR / "documents_index.json"
        if index_file.exists():
            with open(index_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                for doc_data in data.get("documents", []):
                    doc = NormativeDocument(**doc_data)
                    self.documents[doc.id] = doc

    def _save_documents_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        NORMS_DIR.mkdir(parents=True, exist_ok=True)
        index_file = NORMS_DIR / "documents_index.json"

        data = {
            "documents": [
                {
                    "id": doc.id,
                    "code": doc.code,
                    "title": doc.title,
                    "category": doc.category,
                    "status": doc.status,
                    "effective_date": doc.effective_date,
                    "source_file": doc.source_file
                }
                for doc in self.documents.values()
            ]
        }

        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def ingest_pdf(self, file_path: str, title: str = None, status: str = "active") -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É"""
        if not self._initialized:
            logger.error("RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False

        try:
            # –ü–∞—Ä—Å–∏–º PDF
            full_text, pages = NormativePDFParser.parse_pdf(file_path)
            if not full_text:
                return False

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            filename = Path(file_path).name
            info = NormativePDFParser.extract_document_info(full_text, filename)

            # –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç
            doc_id = hashlib.md5(f"{info['code']}_{filename}".encode()).hexdigest()[:16]

            document = NormativeDocument(
                id=doc_id,
                code=info["code"],
                title=title or info["code"],
                category=info["category"],
                status=status,
                effective_date=info.get("effective_date"),
                source_file=filename
            )

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = NormativeChunker.chunk_by_sections(full_text, doc_id, info["code"])

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ vector DB
            added = self.vector_db.add_chunks(chunks)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å
            self.documents[doc_id] = document
            self._save_documents_index()

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç {info['code']}: {added} —á–∞–Ω–∫–æ–≤")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
            return False

    def ingest_text(
        self,
        text: str,
        code: str,
        title: str,
        category: str = "manual",
        status: str = "active"
    ) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É"""
        if not self._initialized:
            logger.error("RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False

        try:
            doc_id = hashlib.md5(f"{code}_{title}".encode()).hexdigest()[:16]

            document = NormativeDocument(
                id=doc_id,
                code=code,
                title=title,
                category=category,
                status=status,
                effective_date=None,
                source_file=None
            )

            chunks = NormativeChunker.chunk_by_sections(text, doc_id, code)
            added = self.vector_db.add_chunks(chunks)

            self.documents[doc_id] = document
            self._save_documents_index()

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç {code}: {added} —á–∞–Ω–∫–æ–≤")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ {code}: {e}")
            return False

    def search(
        self,
        query: str,
        top_k: int = 5,
        category: str = None
    ) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            category: –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–°–ü, –ì–û–°–¢, –°–ù–∏–ü, –§–ó)

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
        """
        if not self._initialized:
            logger.error("RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return []

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä
        filter_dict = None
        if category:
            filter_dict = {"category": category}

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = self.vector_db.search(query, top_k, filter_dict)

        # –û–±–æ–≥–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        enriched_results = []
        for result in results:
            doc_id = result["metadata"].get("document_id")
            document = self.documents.get(doc_id)

            enriched_results.append({
                "content": result["content"],
                "section": result["metadata"].get("section", ""),
                "document_code": result["metadata"].get("document_code", ""),
                "document_title": document.title if document else "",
                "document_category": document.category if document else "",
                "document_status": document.status if document else "",
                "relevance_score": result["relevance_score"]
            })

        return enriched_results

    def get_norm_citation(self, query: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞

        Returns:
            –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞ –∏–ª–∏ None
        """
        results = self.search(query, top_k=1)

        if not results:
            return None

        best = results[0]
        citation = (
            f"üìö **{best['document_code']}**"
            f"{' (–ø. ' + best['section'] + ')' if best['section'] else ''}\n\n"
            f"_{best['content'][:500]}{'...' if len(best['content']) > 500 else ''}_\n\n"
            f"üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {best['relevance_score']:.0%}"
        )

        return citation

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã"""
        db_stats = self.vector_db.get_stats()

        return {
            "initialized": self._initialized,
            "total_documents": len(self.documents),
            "documents_by_category": self._count_by_category(),
            "vector_db": db_stats
        }

    def _count_by_category(self) -> Dict[str, int]:
        """–ü–æ–¥—Å—á—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        counts = {}
        for doc in self.documents.values():
            counts[doc.category] = counts.get(doc.category, 0) + 1
        return counts


# ============================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–†
# ============================================================================

_rag_instance: Optional[NormativeRAG] = None


def get_normative_rag() -> Optional[NormativeRAG]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ RAG"""
    global _rag_instance
    return _rag_instance


def init_normative_rag() -> bool:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ RAG"""
    global _rag_instance

    _rag_instance = NormativeRAG()
    return _rag_instance.initialize()


# ============================================================================
# INTEGRATION API
# ============================================================================

async def search_norms_for_query(query: str, top_k: int = 3) -> List[Dict]:
    """
    API –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤ (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –±–æ—Ç–µ)

    Args:
        query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
    """
    rag = get_normative_rag()
    if not rag:
        return []

    return rag.search(query, top_k)


async def get_norm_for_answer(query: str) -> Optional[str]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç

    Args:
        query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ü–∏—Ç–∞—Ç–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ –∏–ª–∏ None
    """
    rag = get_normative_rag()
    if not rag:
        return None

    return rag.get_norm_citation(query)


def is_rag_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RAG —Å–∏—Å—Ç–µ–º—ã"""
    rag = get_normative_rag()
    return rag is not None and rag._initialized
