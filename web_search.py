"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —Å–∞–π—Ç–∞—Ö —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤ –†–§
"""

import requests
from bs4 import BeautifulSoup
import logging
import re
from typing import Optional, Dict, List
from datetime import datetime

logger = logging.getLogger(__name__)


# === –ü–ê–†–°–ò–ù–ì DOCS.CNTD.RU (–ë–ê–ó–ê –ù–û–†–ú–ê–¢–ò–í–û–í) ===

def search_regulation_cntd(regulation_code: str) -> Optional[Dict]:
    """
    –ü–æ–∏—Å–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ –Ω–∞ docs.cntd.ru

    Args:
        regulation_code: –ö–æ–¥ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–°–ü 63.13330.2018", "–ì–û–°–¢ 31937-2011")

    Returns:
        Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–æ—Ä–º–∞—Ç–∏–≤–µ –∏–ª–∏ None
    """
    try:
        # –û—á–∏—â–∞–µ–º –∫–æ–¥ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_query = regulation_code.replace(" ", "+")
        search_url = f"https://docs.cntd.ru/search?q={search_query}"

        logger.info(f"üîç –ü–æ–∏—Å–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞: {regulation_code}")

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞
        result = soup.find('div', class_='search-result-item')
        if not result:
            logger.warning(f"–ù–æ—Ä–º–∞—Ç–∏–≤ {regulation_code} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ docs.cntd.ru")
            return None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        title_elem = result.find('a', class_='link')
        if not title_elem:
            return None

        title = title_elem.get_text(strip=True)
        link = "https://docs.cntd.ru" + title_elem['href']

        # –ò—â–µ–º —Å—Ç–∞—Ç—É—Å (–¥–µ–π—Å—Ç–≤—É–µ—Ç/–æ—Ç–º–µ–Ω–µ–Ω)
        status_elem = result.find('span', class_='status')
        status = status_elem.get_text(strip=True) if status_elem else "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω"

        # –î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è
        date_pattern = r'—Å\s+(\d{2}\.\d{2}\.\d{4})'
        date_match = re.search(date_pattern, result.get_text())
        valid_from = date_match.group(1) if date_match else None

        result_data = {
            "code": regulation_code,
            "title": title,
            "link": link,
            "status": status,
            "valid_from": valid_from,
            "source": "docs.cntd.ru",
            "search_date": datetime.now().strftime("%Y-%m-%d %H:%M")
        }

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω: {title} ({status})")
        return result_data

    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ docs.cntd.ru: {e}")
        return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ docs.cntd.ru: {e}")
        return None


def get_regulation_text(regulation_url: str, max_chars: int = 5000) -> Optional[str]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ —Å docs.cntd.ru

    Args:
        regulation_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–∞
        max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è

    Returns:
        –¢–µ–∫—Å—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–∞ –∏–ª–∏ None
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(regulation_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content = soup.find('div', class_='document-content')
        if not content:
            content = soup.find('div', id='text')

        if content:
            text = content.get_text(separator='\n', strip=True)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            if len(text) > max_chars:
                text = text[:max_chars] + "..."
            return text

        return None

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞: {e}")
        return None


# === –ü–ê–†–°–ò–ù–ì MINSTROYRF.GOV.RU (–ù–û–í–û–°–¢–ò –ò –ò–ó–ú–ï–ù–ï–ù–ò–Ø) ===

def search_minstroy_news(keywords: List[str], max_results: int = 3) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å–∞–π—Ç–µ –ú–∏–Ω—Å—Ç—Ä–æ—è –†–æ—Å—Å–∏–∏

    Args:
        keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        List —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
    """
    try:
        # URL —Ä–∞–∑–¥–µ–ª–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ú–∏–Ω—Å—Ç—Ä–æ—è
        news_url = "https://minstroyrf.gov.ru/trades/gospolitika/"

        logger.info(f"üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ú–∏–Ω—Å—Ç—Ä–æ—è: {', '.join(keywords)}")

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(news_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏
        news_items = soup.find_all('div', class_='news-item', limit=max_results * 2)

        results = []
        for item in news_items:
            if len(results) >= max_results:
                break

            title_elem = item.find('a')
            if not title_elem:
                continue

            title = title_elem.get_text(strip=True)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if any(keyword.lower() in title.lower() for keyword in keywords):
                date_elem = item.find('time')
                date = date_elem.get_text(strip=True) if date_elem else "–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"

                link = "https://minstroyrf.gov.ru" + title_elem['href']

                results.append({
                    "title": title,
                    "date": date,
                    "link": link,
                    "source": "minstroyrf.gov.ru"
                })

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –Ω–æ–≤–æ—Å—Ç–µ–π –ú–∏–Ω—Å—Ç—Ä–æ—è")
        return results

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ minstroyrf.gov.ru: {e}")
        return []


# === –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ù–ï–û–ë–•–û–î–ò–ú–û–°–¢–ò –ü–û–ò–°–ö–ê ===

def should_perform_web_search(user_message: str) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω—É–∂–µ–Ω –ª–∏ –≤–µ–±-–ø–æ–∏—Å–∫ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å

    Args:
        user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        True –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–∏—Å–∫, False –µ—Å–ª–∏ –Ω–µ—Ç
    """
    # –¢—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
    search_triggers = [
        "–∞–∫—Ç—É–∞–ª—å–Ω",
        "–Ω–æ–≤—ã–π",
        "–Ω–æ–≤–∞—è",
        "—Å–≤–µ–∂–∏–π",
        "–ø–æ—Å–ª–µ–¥–Ω",
        "2025",
        "2026",
        "2027",
        "–∏–∑–º–µ–Ω–µ–Ω–∏",
        "–æ–±–Ω–æ–≤–ª–µ–Ω",
        "–¥–µ–π—Å—Ç–≤—É–µ—Ç",
        "–æ—Ç–º–µ–Ω–µ–Ω",
        "–ø—Ä–æ–≤–µ—Ä—å",
        "–Ω–∞–π–¥–∏",
        "–ø–æ–∏—â–∏"
    ]

    message_lower = user_message.lower()
    return any(trigger in message_lower for trigger in search_triggers)


def extract_regulation_codes(text: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ—á—å –∫–æ–¥—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        List —Å –∫–æ–¥–∞–º–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
    """
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
    patterns = [
        r'–°–ü\s+[\d.]+\.[\d.]+',  # –°–ü 63.13330.2018
        r'–ì–û–°–¢\s+[–†–ï\s]*[\d.-]+',  # –ì–û–°–¢ 31937-2011, –ì–û–°–¢ –† 57580
        r'–°–ù–∏–ü\s+[\d.-]+',  # –°–ù–∏–ü 2.01.07-85
        r'–ü–ü–ë\s+[\d-]+',  # –ü–ü–ë 01-03
    ]

    codes = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        codes.extend(matches)

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    return list(set(codes))


# === –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–û–ò–°–ö–ê ===

def perform_web_search(user_message: str) -> Optional[str]:
    """
    –í—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–±-–ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Args:
        user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏–ª–∏ None
    """
    if not should_perform_web_search(user_message):
        return None

    logger.info(f"üåê –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –≤–µ–±-–ø–æ–∏—Å–∫ –¥–ª—è: {user_message[:100]}...")

    results_text = "üåê **–†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ï–ë-–ü–û–ò–°–ö–ê:**\n\n"
    found_anything = False

    # 1. –ò—â–µ–º —É–ø–æ–º—è–Ω—É—Ç—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã
    regulation_codes = extract_regulation_codes(user_message)
    if regulation_codes:
        results_text += "üìö **–ü–†–û–í–ï–†–ö–ê –ù–û–†–ú–ê–¢–ò–í–û–í:**\n"
        for code in regulation_codes[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –Ω–æ—Ä–º–∞—Ç–∏–≤–∞
            reg_info = search_regulation_cntd(code)
            if reg_info:
                results_text += f"\n‚Ä¢ **{reg_info['code']}**\n"
                results_text += f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {reg_info['title']}\n"
                results_text += f"  –°—Ç–∞—Ç—É—Å: {reg_info['status']}\n"
                if reg_info['valid_from']:
                    results_text += f"  –î–µ–π—Å—Ç–≤—É–µ—Ç —Å: {reg_info['valid_from']}\n"
                results_text += f"  –°—Å—ã–ª–∫–∞: {reg_info['link']}\n"
                found_anything = True
        results_text += "\n"

    # 2. –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ú–∏–Ω—Å—Ç—Ä–æ—è (–µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≥–æ–¥–∞ 2025-2027)
    if any(year in user_message for year in ["2025", "2026", "2027"]):
        keywords = ["–Ω–æ—Ä–º–∞—Ç–∏–≤", "–°–ü", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"]
        news = search_minstroy_news(keywords, max_results=2)
        if news:
            results_text += "üì∞ **–ê–ö–¢–£–ê–õ–¨–ù–´–ï –ù–û–í–û–°–¢–ò –ú–ò–ù–°–¢–†–û–Ø:**\n"
            for item in news:
                results_text += f"\n‚Ä¢ **{item['title']}**\n"
                results_text += f"  –î–∞—Ç–∞: {item['date']}\n"
                results_text += f"  –°—Å—ã–ª–∫–∞: {item['link']}\n"
                found_anything = True
            results_text += "\n"

    if not found_anything:
        return None

    results_text += f"*–ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω: {datetime.now().strftime('%d.%m.%Y %H:%M')}*\n"
    results_text += "*–î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã –Ω–∞ –º–æ–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞*"

    return results_text
